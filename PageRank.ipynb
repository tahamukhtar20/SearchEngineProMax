{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vypv4BvL1cgW"
      },
      "outputs": [],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPbuW3171fxD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, udf, collect_list, explode, sum as spark_sum\n",
        "from pyspark.sql.types import ArrayType, FloatType, BooleanType, StructType, StructField, StringType\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fSVs1BP14CD"
      },
      "source": [
        "**Graph Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR6z6JpH1mDS"
      },
      "outputs": [],
      "source": [
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.graph = {}\n",
        "\n",
        "    def add_edge(self, u, v):\n",
        "        if u in self.graph:\n",
        "            self.graph[u].append(v)\n",
        "        else:\n",
        "            self.graph[u] = [v]\n",
        "\n",
        "    def __str__(self):\n",
        "        result = \"\"\n",
        "        for node, neighbors in self.graph.items():\n",
        "            result += f\"{node}: {neighbors}\\n\"\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwnORbd118qi"
      },
      "source": [
        "**Graph Builder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00zMbmYy1ozS"
      },
      "outputs": [],
      "source": [
        "def build_graph_from_json_folder(folder_path):\n",
        "    graph = Graph()\n",
        "    print(\"Graph building started\")\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".json\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, 'r') as file:\n",
        "                data = json.load(file)\n",
        "                page_name = data.get('url')\n",
        "                forward_links = data.get('forwardLinks', [])\n",
        "                if not forward_links:\n",
        "                    forward_links = [None]\n",
        "                for link in forward_links:\n",
        "                    graph.add_edge(page_name, link)\n",
        "    print(\"Graph building ended\")\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJqa5NlA2DNa"
      },
      "source": [
        "**Page Rank**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeJacehx1q8i"
      },
      "outputs": [],
      "source": [
        "def rank_dist(link_list, rank):\n",
        "    len_link_list = len(link_list)\n",
        "    if len_link_list > 0 and link_list[0] is not None:\n",
        "        rank = rank / len_link_list\n",
        "        r_list = [(x, rank) for x in link_list]\n",
        "    else:\n",
        "        r_list = [(\"DANGLING\", rank)]\n",
        "    return r_list\n",
        "\n",
        "inner_schema = StructType([\n",
        "    StructField(\"uri_id\", StringType(), False),\n",
        "    StructField(\"rank\", FloatType(), False)\n",
        "])\n",
        "\n",
        "ranks_dist_udf = udf(rank_dist, ArrayType(inner_schema))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GruUEf0m1t8r"
      },
      "outputs": [],
      "source": [
        "def run(graph, alpha=0.15, convergence=0.01):\n",
        "    edge_info = [(src, dst) for src, dst_list in graph.graph.items() for dst in dst_list]\n",
        "    spark = SparkSession.builder.appName(\"PageRank\").getOrCreate()\n",
        "    print(\"Start Spark session\")\n",
        "    edges_df = spark.createDataFrame(edge_info, ['src', 'dst'])\n",
        "\n",
        "    dataframe_ranked = edges_df\\\n",
        "        .groupby('src')\\\n",
        "        .agg(collect_list('dst').alias('dst_list'))\\\n",
        "        .withColumnRenamed('src', 'uri_id')\n",
        "\n",
        "    dataframe_ranked = dataframe_ranked.withColumn('rank', lit(1.0 / dataframe_ranked.count()))\n",
        "\n",
        "    n_nodes = dataframe_ranked.count()\n",
        "    i = 0\n",
        "    checkpoint_dir = 'checkpoint_directory'  # Ensure this directory exists\n",
        "    spark.sparkContext.setCheckpointDir(checkpoint_dir)\n",
        "    while True:\n",
        "        dataframe_ranked.cache()\n",
        "\n",
        "        ranks_one_df = dataframe_ranked.withColumn(\n",
        "            'link_map_pr', ranks_dist_udf('dst_list', 'rank'))\n",
        "        ranks_one_df = ranks_one_df.select(\n",
        "            explode('link_map_pr').alias('exploded'))\n",
        "        ranks_one_df = ranks_one_df\\\n",
        "            .withColumn('dst_id', ranks_one_df['exploded'].getItem('uri_id'))\\\n",
        "            .withColumn('rank_i', ranks_one_df['exploded'].getItem('rank'))\\\n",
        "            .drop('exploded')\n",
        "\n",
        "        ranks_one_df = ranks_one_df\\\n",
        "            .groupby('dst_id')\\\n",
        "            .sum('rank_i')\\\n",
        "            .withColumnRenamed('sum(rank_i)', 'rank_i')\n",
        "\n",
        "        dataframe_ranked = dataframe_ranked\\\n",
        "            .join(ranks_one_df, dataframe_ranked['uri_id'] == ranks_one_df['dst_id'], 'outer')\\\n",
        "            .drop('dst_id')\n",
        "\n",
        "        dangling_rank = dataframe_ranked\\\n",
        "            .filter(dataframe_ranked.uri_id == \"DANGLING\")\\\n",
        "            .select(spark_sum('rank_i'))\\\n",
        "            .first()[0]\n",
        "\n",
        "        dataframe_ranked = dataframe_ranked.filter(dataframe_ranked.uri_id != \"DANGLING\")\n",
        "\n",
        "        if dangling_rank:\n",
        "            dist_alpha = ((dangling_rank / n_nodes) * (1 - alpha)) + alpha\n",
        "        else:\n",
        "            dist_alpha = alpha\n",
        "\n",
        "        sum_alpha_and_pr_udf = udf(lambda x: (x * (1 - alpha)) + dist_alpha, FloatType())\n",
        "        dataframe_ranked = dataframe_ranked.na.fill(0, ['rank_i'])\n",
        "        dataframe_ranked = dataframe_ranked.withColumn('rank_i', sum_alpha_and_pr_udf('rank_i'))\n",
        "\n",
        "        convergence_udf = udf(lambda rank_i, rank: abs(rank_i - rank) <= convergence, BooleanType())\n",
        "        dataframe_ranked = dataframe_ranked.withColumn('convergence', convergence_udf('rank', 'rank_i'))\n",
        "        count_not_converged = dataframe_ranked.filter(dataframe_ranked.convergence == False).count()\n",
        "        dataframe_ranked = dataframe_ranked.drop('convergence').drop('rank').withColumnRenamed('rank_i', 'rank')\n",
        "\n",
        "        dataframe_ranked = dataframe_ranked.checkpoint()\n",
        "\n",
        "        if count_not_converged == 0:\n",
        "            print(\"All nodes convergend according to the given criteria\")\n",
        "            break\n",
        "        else:\n",
        "            print(f'Nodes not yet converged: {count_not_converged}')\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    output_graph = {}\n",
        "    for row in dataframe_ranked.collect():\n",
        "        output_graph[row['uri_id']] = row['rank']\n",
        "\n",
        "    return output_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viEfJqJq2Jii"
      },
      "source": [
        "**Driver Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D08g_15c10qa"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    spark = SparkSession.builder.appName(\"PageRank\").getOrCreate()\n",
        "    folder_path = \"folder\"  # Update this with your folder containing JSON files\n",
        "    graph = build_graph_from_json_folder(folder_path)\n",
        "    print(\"Start ranking\")\n",
        "    ranked_graph = run(graph)\n",
        "    print(\"PageRank Results:\")\n",
        "    print(ranked_graph)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
